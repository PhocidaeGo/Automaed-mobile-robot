{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227e82fb-c57d-4b80-9911-f66c004c287c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Reading and visualizing raw point clouds using `Data` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a96951-efec-4fc2-be89-d2a864bad441",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.1. Preparing a `Data` reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540aef5c-3f9e-41ae-8c8a-c557e307c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the project's files to the Python path\n",
    "file_path = os.path.dirname(os.path.abspath(''))  # for .ipynb notebook\n",
    "sys.path.append(file_path)\n",
    "\n",
    "import torch\n",
    "from plyfile import PlyData\n",
    "from src.data import Data\n",
    "from src.utils.color import to_float_rgb\n",
    "\n",
    "def read_ply_file(\n",
    "        filepath, \n",
    "        xyz=True, \n",
    "        rgb=True, \n",
    "        intensity=False, \n",
    "        semantic=False, \n",
    "        instance=False):\n",
    "    \"\"\"Read a PLY file.\n",
    "\n",
    "    :param filepath: str\n",
    "        Absolute path to the PLY file\n",
    "    :param xyz: bool\n",
    "        Whether XYZ coordinates should be saved in the output Data.pos\n",
    "    :param rgb: bool\n",
    "        Whether RGB colors should be saved in the output Data.rgb\n",
    "    :param intensity: bool\n",
    "        Whether intensity should be saved in the output Data.intensity\n",
    "    :param semantic: bool\n",
    "        Whether semantic labels should be saved in the output Data.y\n",
    "    :param instance: bool\n",
    "        Whether instance labels should be saved in the output Data.obj\n",
    "    \"\"\"\n",
    "    # Create an empty Data object\n",
    "    data = Data()\n",
    "    \n",
    "    plydata = PlyData.read(filepath)\n",
    "    vertices = plydata['vertex']\n",
    "\n",
    "    # Populate data with point coordinates\n",
    "    if xyz:\n",
    "        data.pos = torch.stack([\n",
    "            torch.tensor(vertices[axis], dtype=torch.float32)\n",
    "            for axis in ['x', 'y', 'z']\n",
    "        ], dim=-1)\n",
    "\n",
    "    # Populate data with point RGB colors\n",
    "    if rgb and 'red' in vertices and 'green' in vertices and 'blue' in vertices:\n",
    "        data.rgb = to_float_rgb(torch.stack([\n",
    "            torch.tensor(vertices[axis], dtype=torch.float32) / 255\n",
    "            for axis in ['red', 'green', 'blue']\n",
    "        ], dim=-1))\n",
    "\n",
    "    # Populate data with point LiDAR intensity\n",
    "    if intensity and 'intensity' in vertices:\n",
    "        data.intensity = torch.tensor(vertices['intensity'], dtype=torch.float32)\n",
    "\n",
    "    # Populate data with point semantic segmentation labels\n",
    "    if semantic and 'label' in vertices:\n",
    "        data.y = torch.tensor(vertices['label'], dtype=torch.long)\n",
    "\n",
    "    # Populate data with point panoptic segmentation labels\n",
    "    if instance and 'instance' in vertices:\n",
    "        data.obj = torch.tensor(vertices['instance'], dtype=torch.long)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b30b8e-9cd1-42f0-aa48-15d889e6d631",
   "metadata": {},
   "source": [
    "Often, we need to remap the raw labels provided in a dataset to another set of labels to be used for training. \n",
    "In the next cell, we define some environment variables for remapping Vancouver class indices and corresponding customized class names and colors for downstream visualization.\n",
    "\n",
    "> **Tip ðŸ’¡**: As described in our [datasets documentation](../docs/datasets.md/#semantic-label-format) we consider labels in `[0, num_classes - 1]` to be valid classes and use the `num_classes` label for void/ignored/unlabeled points (whichever you call it). Check out the [documentation](../docs/datasets.md/#semantic-label-format) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4b4c2e-8d2b-4bf7-be1f-ea9b48077523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of classes in the dataset (excluding void/unlabeled/ignored)\n",
    "PLY_NUM_CLASSES = 13\n",
    "\n",
    "# Mapping from original classes\n",
    "ID2TRAINID = np.asarray([\n",
    "    0,   # 0 Ceiling\n",
    "    1,   # 1 Floor\n",
    "    2,   # 2 Wall\n",
    "    3,   # 3 Beam\n",
    "    4,   # 4 Column\n",
    "    5,   # 5 Window\n",
    "    6,   # 6 Door\n",
    "    7,   # 7 Chair\n",
    "    8,   # 8 Table\n",
    "    9,   # 9 Bookcase\n",
    "    10,  # 10 Sofa\n",
    "    11,  # 11 Board\n",
    "    12,  # 12 Clutter\n",
    "    13   # 13 Ignored\n",
    "])\n",
    "\n",
    "# Class names (including void/unlabeled/ignored last)\n",
    "PLY_CLASS_NAMES = [\n",
    "    'ceiling',\n",
    "    'floor',\n",
    "    'wall',\n",
    "    'beam',\n",
    "    'column',\n",
    "    'window',\n",
    "    'door',\n",
    "    'chair',\n",
    "    'table',\n",
    "    'bookcase',\n",
    "    'sofa',\n",
    "    'board',\n",
    "    'clutter',\n",
    "    'ignored']\n",
    "\n",
    "# Class color palette (including void/unlabeled/ignored last)\n",
    "PLY_CLASS_COLORS = np.asarray([\n",
    "    [233, 229, 107],  # Ceiling\n",
    "    [ 95, 156, 196],  # Floor\n",
    "    [179, 116,  81],  # Wall\n",
    "    [241, 149, 131],  # Beam\n",
    "    [ 81, 163, 148],  # Column\n",
    "    [ 77, 174,  84],  # Window\n",
    "    [108, 135,  75],  # Door\n",
    "    [ 41,  49, 101],  # Chair\n",
    "    [ 79,  79,  76],  # Table\n",
    "    [223,  52,  52],  # Bookcase\n",
    "    [ 89,  47,  95],  # Sofa\n",
    "    [ 81, 109, 114],  # Board\n",
    "    [233, 233, 229],  # Clutter\n",
    "    [  0,   0,   0]])  # Ignored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e1cba-0171-4b7e-9f94-c8d5eb68a1c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.2. `Data` visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fcdbba-77f6-46dd-a7d3-ff722ffa9886",
   "metadata": {},
   "source": [
    "We can now download tiles from [Vancouver LiDAR 2022](https://opendata.vancouver.ca/explore/dataset/lidar-2022/map/?location=12,49.25672,-123.14434) and read their content into a `Data` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b330d9-5390-49ab-a50e-7e2648e357ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/yuanyan/autonomous-exploration-with-lio-sam/maps/test.ply'\n",
    "data = read_ply_file(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2408734-1519-4e9e-9c2b-86c537e45ecc",
   "metadata": {},
   "source": [
    "We have created a `Data` object containing out point cloud and associated attributes. \n",
    "Let's have a closer look at it !\n",
    "\n",
    "The basic `Data.__repr__()` will show the attributes (ie keys) in Data and their respective shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a211df2-5ae2-44a2-8457-33e1b5b2bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fa630-639a-4ae2-af2e-b6abaff9dd58",
   "metadata": {},
   "source": [
    "You can check the number of points (ie nodes) in a `Data` object with `data.num_points` (or `data.num_nodes`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a948af-4e95-4636-b3a1-6264fba13dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.num_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebf559-d70b-437b-bbc8-b18b11bc5152",
   "metadata": {},
   "source": [
    "You can check the list of attributes stored in a `Data` object with `data.keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c278cd-92a7-47ff-bf57-d3eceb15476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6f14c-9862-42ea-adc2-1ba529618b51",
   "metadata": {},
   "source": [
    "We provide a [Plotly](https://plotly.com/python)-based too for visalizing `Data` objects. To use it, simply use `data.show()`. This function offers many options for customizing your plot. We will see later on that it can also be used for visualizing hierarchical superpoint partitions held in `NAG` objects.\n",
    "\n",
    "First, let's visualize the whole point cloud contained in `Data` (this may take a couple of seconds if your cloud has $\\sim10^5$ points or more).\n",
    "We can specify our `class_names` and `class_colors` to `show()` to customize the displaying of semantic segmentation labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37a2ec-4149-4af9-ae3f-938bd5b9729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(class_names=PLY_CLASS_NAMES, class_colors=PLY_CLASS_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cc2c77-9121-4e05-8d14-34b2b93a17d3",
   "metadata": {},
   "source": [
    "By default, the point cloud is subsampled`max_points=50000` to alleviate the visualization computation time.\n",
    "To get a clearer, high-resolution view, you can increase `max_points` or visualize smaller scenes.\n",
    "You can for instance, only display a spherical crop of the point cloud by specifying a `center` and a `radius`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca61c3c-5900-4807-896a-1d018f2f40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(center=[-17, 36, 0], radius=30, keys=['intensity'], class_names=PLY_CLASS_NAMES, class_colors=PLY_CLASS_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39655658-791f-411e-8c41-b6d9d92a90ba",
   "metadata": {},
   "source": [
    "> **Tips ðŸ’¡**\n",
    "> - More info on our `Data` structure ? ðŸ‘‰ see [`docs/data_structures.md`](../docs/data_structures.md), our source code in `src.data.data`, and the [PyG Data documentation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data) it builds upon\n",
    "> - More info on our `show()` visualization tool ? ðŸ‘‰ see [`docs/visualization.md`](../docs/visualization.md) and  source code in `src.visualization`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04e913-8bd0-4a56-99ff-c27f6fd5b2be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Using a pretrained model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa568587-4408-481d-b533-d8990d791402",
   "metadata": {},
   "source": [
    "We provide pretrained weights and preprocessing parametrization for several datasets (see [README](../README.md) and [datasets documentation](../docs/datasets.md)). Since the Vancouver dataset is fairly similar to DALES, we would like to check how a DALES-pretrained SPT would fare on our present `Data` object.\n",
    "\n",
    "As mentioned in the [introductory slides](../media/superpoint_transformer_tutorial.pdf), running an inference with a pretrained SPT requires more than just the model weights. Indeed, we also need to apply to our `Data` the same `pre_transform` and `on_device_transform` as the ones used for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5b9e6-aad2-47c5-a6cb-c0d776fb8133",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.1. Instantiating transforms from `configs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37863cae-bd91-461c-b5b9-1cd59c56aed7",
   "metadata": {},
   "source": [
    "We will first need to recover the transforms used in the DALES experiments as provided in the `configs/experiment` using [Hydra](https://hydra.cc/docs/intro/). \n",
    "In the next cell, we show how to use the `init_config()` utility to get the **exact configuration used for training the released DALES model**.\n",
    "\n",
    "> **Tips ðŸ’¡**\n",
    "> - More info on how `configs/` & [Hydra](https://hydra.cc/docs/intro/) work ? ðŸ‘‰ see the [lightning-hydra-template](https://github.com/ashleve/lightning-hydra-template) repository\n",
    "> - More info on a specific experiment's settings ? ðŸ‘‰ explore our configuratin files in `configs/`, these are fairly commented ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79b45daf-4ced-4eed-ab3f-fbc1b7a2f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import init_config\n",
    "\n",
    "cfg = init_config(overrides=[f\"experiment=semantic/s3dis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e129c-8a95-4a27-9133-292a341aeeac",
   "metadata": {},
   "source": [
    "This `cfg` is an [omegaconf](https://omegaconf.readthedocs.io) `DictConfig` object. It contains all the necessary hyperparameters for reproducing the pretraining experiment: dataset, model structure, training recipe, etc. We can explore its content just like a basic dictionary, or a simple object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864cd81-c4f3-4ec0-a628-fdbb5e4787c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f56c8b-935d-4cd8-ad1e-9a61dab72dba",
   "metadata": {},
   "source": [
    "The parametrization of the transforms is specified in the datamodule config in `cfg.datamodule`.\n",
    "We can instantiate the transforms from an [omegaconf](https://omegaconf.readthedocs.io) `DictConfig` object without instantiating the whole dataset by using the `instantiate_datamodule_transforms()` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade93e52-97ac-4e86-9a40-c3c08f9e0c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transforms import instantiate_datamodule_transforms\n",
    "\n",
    "transforms_dict = instantiate_datamodule_transforms(cfg.datamodule)\n",
    "transforms_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258199eb-dd95-4769-b5c0-b6ea2e38edb2",
   "metadata": {},
   "source": [
    "The transforms are chained operations applied to a `Data` or a `NAG` object. Their order and parametrization plays a significant role and modifying these may have non-negligible downstream effects. **These must be thought as part of the model itself**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d0dbf9-62ca-45b6-b5e4-40ff827bf25d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2. Applying transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268c002-8179-4900-9ce0-3a16cb8f89e8",
   "metadata": {},
   "source": [
    "As explained in the [introductory slides](../media/superpoint_transformer_tutorial.pdf), we will be using `pre_transform` and `on_device_test_transform` to reproduce the behavior of the pretrained model at inference time.\n",
    "\n",
    "> **Note ðŸ¤“**: In the next cell, we manually apply some `NAGRemoveKeys()` transform after the `pre_transform`. This is because we ocasionally need to mimick the full behavior of the pretraining `Dataset`: after the `pre_transform` is executed, the preprocessed `NAG` is saved to disk. When later read from disk by the `Dataset`, only the `point_load_keys` attributes of `NAG[0]` and `segment_load_keys` attributes of `NAG[i], i>0` are loaded from disk. This mechanism ensures we only load the strict necessary during training, hence saving I/O time. Since we are running the `pre_transform` manually here, we need to account for this mechanism and discard the preprocessed attributes that the DALES dataset did not read from disk. These can be found in `cfg.datamodule.point_load_keys` and `cfg.datamodule.segment_load_keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f41463ce-5f93-4a60-a276-a759f638f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-transforms\n",
    "nag = transforms_dict['pre_transform'](data)\n",
    "\n",
    "# Simulate the behavior of the dataset's I/O behavior with only\n",
    "# `point_load_keys` and `segment_load_keys` loaded from disk\n",
    "from src.transforms import NAGRemoveKeys\n",
    "nag = NAGRemoveKeys(level=0, keys=[k for k in nag[0].keys if k not in cfg.datamodule.point_load_keys])(nag)\n",
    "nag = NAGRemoveKeys(level='1+', keys=[k for k in nag[1].keys if k not in cfg.datamodule.segment_load_keys])(nag)\n",
    "\n",
    "# Move to device\n",
    "nag = nag.cuda()\n",
    "\n",
    "# Apply on-device transforms\n",
    "nag = transforms_dict['on_device_test_transform'](nag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a83c80-844d-4f71-ae85-4b1fcfbdce9d",
   "metadata": {},
   "source": [
    "The output of the transforms is no longer a `Data` object, but a `NAG`. This is the data structure we use to carry around **point clouds** and **hierarchical superpoint partitions**. \n",
    "\n",
    "Essentially, it is a list of `Data` objects, each representing a partition level:\n",
    "- `nag[0]` is $P_0$, the (voxelized) points\n",
    "- `nag[i]` is $P_i$, the $\\text{i}^\\text{th}$ superpoint partition level \n",
    "\n",
    "At each level $i>0$, the `edge_index` and `edge_attr` attributes carry the **superpoint adjacency graph** and corresponding **adjacency features**.\n",
    "\n",
    "> **Tip ðŸ’¡** More info on our `NAG` structure ? ðŸ‘‰ see [`docs/data_structures.md`](../docs/data_structures.md) and source code in `src.data.nag`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221bacf-a499-479d-9468-cfada4e0b3d9",
   "metadata": {},
   "source": [
    "Now we have preprocessed our data, we need to run an inference with the pretrained model.\n",
    "\n",
    "> **Tip ðŸ’¡**: If you want to store your progress disk, both `Data` and `NAG` have `.save()` and `.load()` methods specially designed with fast I/O and disk usage in mind ðŸ˜‰."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847c821-855d-4913-9e4b-a35c200203e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.3. Instantiating a pretrained model from `configs/` and a `*.ckpt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066d2505-dc66-439b-b7f0-02d4267fd2e1",
   "metadata": {},
   "source": [
    "Similar to the transforms, we will use the DALES experiment configuration files to instantiate the **pretrained model**. \n",
    "This time, the part of the [omegaconf](https://omegaconf.readthedocs.io) `DictConfig` object we are interested in is stored under `cfg.model`.\n",
    "\n",
    "As stated in the [README](../README.md), the pretrained weights for our models can be recovered from [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8042712.svg)](https://doi.org/10.5281/zenodo.8042712)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89222181-655a-4a3b-a9f2-2ab2983f9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra \n",
    "from src.utils import init_config\n",
    "\n",
    "# Path to the checkpoint file downloaded from https://zenodo.org/records/8042712\n",
    "ckpt_path = \"/home/yuanyan/Documents/superpoint_transformer/models/spt-2_s3dis_fold6.ckpt\"\n",
    "\n",
    "cfg = init_config(overrides=[f\"experiment=semantic/s3dis\"])\n",
    "\n",
    "# Instantiate the model and load pretrained weights\n",
    "model = hydra.utils.instantiate(cfg.model)\n",
    "model = model._load_from_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241d311-5843-4ad0-a48b-463d33be255c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.4. Applying SPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef6135-8b2e-499c-937f-4e3ee8afe5bc",
   "metadata": {},
   "source": [
    "Now everything is ready for running our inference ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "143c799f-f7a5-4a2f-8a4d-c378191a0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model in inference mode on the same device as the input\n",
    "model = model.eval().to(nag.device)\n",
    "\n",
    "# Inference, returns a task-specific ouput object carrying predictions\n",
    "with torch.no_grad():\n",
    "    output = model(nag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4712247-4af5-4a5f-b7cc-b75c4f7372aa",
   "metadata": {},
   "source": [
    "The output of the model is a `SemanticSegmentationOutput` object. It is a simple class dedicated to holding onto predictions in `output.semantic_pred()` and facilitating certain basic post-processing operations such as metrics computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48bc881-aa88-435d-b989-8dc4af7cdbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.semantic_pred().shape, nag.num_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c1af4-a6c9-4f14-a553-8ed46098c417",
   "metadata": {},
   "source": [
    "As stated in [introductory slides](../media/superpoint_transformer_tutorial.pdf), it is important to remember that, by default, **SPT outputs predictions on the $P_1$ level** (ie `nag[1]`). Since the superpoints $P_1$ are assumed to be semantically pure, simply classifying those is equivalent to classifying each point in the scene. In doing so, we save a lot of computation and memory during training.\n",
    "\n",
    "Yet, at inference time, we often want the predictions at the voxel level $P_0$ (ie `nag[0]`) or even at the full-resolution of the raw input cloud. \n",
    "To this end, we simply need to distribute the $P_1$ predictions to the lower partition levels.\n",
    "The `SemanticSegmentationOutput.voxel_semantic_pred()` and `SemanticSegmentationOutput.full_res_semantic_pred()` were designed just for that ! \n",
    "\n",
    "In the next cell, we will convert $P_1$ predictions into $P_0$ predictions.\n",
    "\n",
    "> **Tip ðŸ’¡**: For **full-resolution predictions**, see our [`demo.ipynb` notebook](../notebooks/demo.ipynb), and have a look at [`src.utils.output_semantic.py`](../src/utils/output_semantic.py#L140). Remember that if you have applied a tiling to your data, your full-resolution predictions will be given for the tile at hand and not the original point cloud.\n",
    "\n",
    "> **Note ðŸ¤“**: Although SPT does make predictions as $P_1$ node classifications, all losses and metrics are properly computed so as to take into account the true labels assigned to full-resolution points. To make these efficient, our pipeline always tracks the **histogram of ground truth labels** for each voxel in $P_0$ and superpoint in $P_i, i>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51f59a31-3887-4c6a-98b4-5aa54ee97c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the level-0 (voxel-wise) semantic segmentation predictions \n",
    "# based on the predictions on level-1 superpoints and save those for \n",
    "# visualization in the level-0 Data under the 'semantic_pred' attribute\n",
    "nag[0].semantic_pred = output.voxel_semantic_pred(super_index=nag[0].super_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625b77c8-21a6-42d5-ae51-7271c551b07e",
   "metadata": {},
   "source": [
    "Let's visualize the resulting predictions on a small area for high-resolution display.\n",
    "\n",
    "Note that since the model was trained on DALES classes, the predicted labels do not align with those of our Vancouver dataset. \n",
    "For better visualization, we will use the DALES `CLASS_NAMES` and `CLASS_COLORS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d31e7d-5f8f-44db-9765-7d426dfaca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.s3dis_room import CLASS_NAMES as CLASS_NAMES\n",
    "from src.datasets.s3dis_room import CLASS_COLORS as CLASS_COLORS\n",
    "\n",
    "print(CLASS_NAMES)\n",
    "#print(CLASS_COLORS)\n",
    "\n",
    "nag.show(class_names=CLASS_NAMES, class_colors=CLASS_COLORS, center=[0, 0, 0], radius=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f823db5",
   "metadata": {},
   "source": [
    "### 2.5. Post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c6e44",
   "metadata": {},
   "source": [
    "To adapt the pre-trained model to simple construction site environment, we use color remapping here, where the specified labels (wall, beam, column, window, door, bookcase, board) are set to red, and others are set to grey, if bool simple_env is True.\n",
    "\n",
    "It also saves the segmented point cloud to a PLY file with points colored based on their labels for way points generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474246d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# Function to save segmented point cloud with remapping if required\n",
    "def save_segmented_ply(data, file_path, simple_env=False):\n",
    "    points = data.pos.cpu().numpy()  # Extract point coordinates\n",
    "    labels = data.semantic_pred.cpu().numpy()  # Extract semantic labels\n",
    "    \n",
    "    # Map labels to colors\n",
    "    colors = np.array(PLY_CLASS_COLORS)[labels]\n",
    "    \n",
    "    if simple_env:\n",
    "        # Remap specific classes to red, others to grey\n",
    "        red_color = np.array([255, 0, 0])\n",
    "        grey_color = np.array([128, 128, 128])\n",
    "        remap_classes = ['wall', 'beam', 'column', 'window', 'door', 'bookcase', 'board']\n",
    "        remap_indices = [PLY_CLASS_NAMES.index(cls) for cls in remap_classes]\n",
    "        colors = np.where(\n",
    "            np.isin(labels, remap_indices)[:, None], red_color, grey_color\n",
    "        )\n",
    "    \n",
    "    # Create an Open3D point cloud object\n",
    "    pcl = o3d.geometry.PointCloud()\n",
    "    pcl.points = o3d.utility.Vector3dVector(points)\n",
    "    pcl.colors = o3d.utility.Vector3dVector(colors / 255.0)  # Normalize colors to [0, 1]\n",
    "    \n",
    "    # Save to PLY file\n",
    "    o3d.io.write_point_cloud(file_path, pcl)\n",
    "    print(f\"Segmented point cloud saved to {file_path}\")\n",
    "\n",
    "# Example usage\n",
    "output_path = '/home/yuanyan/autonomous-exploration-with-lio-sam/segmented_output.ply'  # Replace with desired file path\n",
    "save_segmented_ply(data=nag[0], file_path=output_path, simple_env=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
